{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from nameparser import HumanName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read bipartite graph and extract names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4054832"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipartite_all = nx.read_gpickle(os.getcwd() + '/Pickle/bipartite-all010420.pickle')\n",
    "bipartite_all.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes from *bipartite_all* have the following attributes:\n",
    "1. degree: int\n",
    "2. name: string\n",
    "3. is_trusted: bool\n",
    "4. is_employee: bool\n",
    "5. bipartite: int\n",
    "\n",
    "Access: bipartite_all.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'degree': 17,\n",
       " 'name': 'JOSEPH GALLO',\n",
       " 'is_trustee': True,\n",
       " 'is_employee': True,\n",
       " 'bipartite': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipartite_all.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'JOSEPH GALLO'),\n",
       " (1, 'EMMA DUNCH'),\n",
       " (2, 'DONALD BORROR'),\n",
       " (3, 'LINDA MOFFITT'),\n",
       " (4, \"M'LISS DORRANCE\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipartite_names = list(bipartite_all.nodes(data = 'name'))  # extract only names\n",
    "bipartite_names[: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read LittleSis names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205548"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "littlesis_names = {}\n",
    "\n",
    "with open(os.getcwd() + '/Pickle/all_people.pickle', 'rb') as pkl:\n",
    "    littlesis_names = pickle.load(pkl)\n",
    "    \n",
    "len(littlesis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Roland A Hernandez'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "littlesis_names['1024']  # key is an ent ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNames(names_dict):\n",
    "    clean_names = {}\n",
    "    error_count, error_list = 0, []\n",
    "    \n",
    "    for name_id in names_dict:\n",
    "        try:\n",
    "            name = names_dict[name_id].strip().upper()\n",
    "\n",
    "            human_name = HumanName(name)\n",
    "\n",
    "            names_list = [human_name.first, human_name.middle, human_name.last]\n",
    "            names_list = [n for n in names_list if len(n) > 0]\n",
    "\n",
    "            if type(name_id) == str:  # convert string ent IDs to int\n",
    "                name_id = int(name_id)\n",
    "                \n",
    "            clean_names[name_id] = ' '.join(names_list)\n",
    "        \n",
    "        except:\n",
    "            error_count += 1\n",
    "            error_list.append(name_id)\n",
    "            \n",
    "#     print(error_count, error_list[: 3])\n",
    "    print(error_count)\n",
    "    return clean_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clean_littlesis_names = cleanNames(littlesis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROLAND A HERNANDEZ'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_littlesis_names[1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "max_num = 1000  # max number of names from bipartite_all, change this value to change the size of the bipartite_all subset!\n",
    "bipartite_names_temp = {item[0]: item[1] for item in bipartite_names[: max_num] if item[1] is not None}  # create dict with ID:name for bipartite-all people if their name is not None\n",
    "clean_bipartite_names = cleanNames(bipartite_names_temp)\n",
    "clean_bipartite_names_reverse = {clean_bipartite_names[i]: i for i in clean_bipartite_names}\n",
    "remaining_bipartite = len(clean_bipartite_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205548 740 740\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_littlesis_names), len(clean_bipartite_names), len(clean_bipartite_names_reverse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting 100 out of 740\n",
      "Inserting 200 out of 740\n",
      "Inserting 300 out of 740\n",
      "Inserting 400 out of 740\n",
      "Inserting 500 out of 740\n",
      "Inserting 600 out of 740\n",
      "Inserting 700 out of 740\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "lsh = MinHashLSH(threshold = 0.5, num_perm = 128)\n",
    "\n",
    "for name_id in clean_bipartite_names:\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print('Inserting', count, 'out of', remaining_bipartite)\n",
    "    \n",
    "    p = MinHash(num_perm = 128)\n",
    "    \n",
    "    name = clean_bipartite_names[name_id]\n",
    "    \n",
    "    for name_split in name.split():\n",
    "        p.update(name_split.encode('utf-8'))\n",
    "        \n",
    "    lsh.insert(name, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 12604 names from bipartite-all\n"
     ]
    }
   ],
   "source": [
    "del clean_bipartite_names\n",
    "print('Inserted', count, 'names from bipartite-all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareNames(person1, person2):\n",
    "    same_person_flag = False\n",
    "    \n",
    "    if person1 != person2:\n",
    "        names1 = person1.split()\n",
    "        names2 = person2.split()\n",
    "        \n",
    "        common_names = list(set(names1) & set(names2))  # intersection\n",
    "        \n",
    "        # we need names of min len = 3 to be same\n",
    "        common_names_min_len = [n for n in common_names if len(n) >= 3]\n",
    "        \n",
    "        neither = None\n",
    "        \n",
    "        try:\n",
    "            if len(common_names_min_len) > 1:  # if at least 2 names satisfy min len condition\n",
    "                if len(common_names) == len(names1) or len(common_names) == len(names2):\n",
    "                    same_person_flag = True\n",
    "\n",
    "                elif (len(names1) - len(common_names) == 1 or len(names2) - len(common_names) == 1) and (len(names1) - len(common_names) <= 2 or len(names2) - len(common_names) <= 2) and names1[0] == names2[0] and names1[-1] == names2[-1]:\n",
    "                    neither = list(set(names1).union(set(names2)) - (set(names1) & set(names2)))  # neither i.e. complement = union - intersection\n",
    "\n",
    "                    if len(neither) == 2 and (len(neither[0]) == 1 or len(neither[1] == 1)) and neither[0][0] == neither[1][0]:\n",
    "                        same_person_flag = True\n",
    "\n",
    "        except:\n",
    "            print(neither, type(neither), person1)\n",
    "    \n",
    "#     ???\n",
    "#     elif person1 == person2:\n",
    "#         same_person_flag = True\n",
    "    \n",
    "    return same_person_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 500\n",
      "Comparing 1000\n",
      "Comparing 1500\n",
      "Comparing 2000\n",
      "Comparing 2500\n",
      "Comparing 3000\n",
      "Comparing 3500\n",
      "Comparing 4000\n",
      "Comparing 4500\n",
      "Comparing 5000\n",
      "Comparing 5500\n",
      "Comparing 6000\n",
      "Comparing 6500\n",
      "Comparing 7000\n",
      "Comparing 7500\n",
      "Comparing 8000\n",
      "Comparing 8500\n",
      "Comparing 9000\n",
      "Comparing 9500\n",
      "Comparing 10000\n",
      "Comparing 10500\n",
      "Comparing 11000\n",
      "Comparing 11500\n",
      "Comparing 12000\n",
      "Comparing 12500\n",
      "['BRIAN', 'L'] <class 'list'> DOUGLAS BRIAN PETERSON\n",
      "Comparing 13000\n",
      "Comparing 13500\n",
      "Comparing 14000\n",
      "Comparing 14500\n",
      "Comparing 15000\n",
      "Comparing 15500\n",
      "Comparing 16000\n",
      "Comparing 16500\n",
      "Comparing 17000\n",
      "Comparing 17500\n",
      "Comparing 18000\n",
      "Comparing 18500\n",
      "Comparing 19000\n",
      "Comparing 19500\n",
      "Comparing 20000\n",
      "Comparing 20500\n",
      "Comparing 21000\n",
      "Comparing 21500\n",
      "Comparing 22000\n",
      "Comparing 22500\n",
      "Comparing 23000\n",
      "Comparing 23500\n",
      "Comparing 24000\n",
      "Comparing 24500\n",
      "Comparing 25000\n",
      "Comparing 25500\n",
      "Comparing 26000\n",
      "Comparing 26500\n",
      "Comparing 27000\n",
      "Comparing 27500\n",
      "Comparing 28000\n",
      "Comparing 28500\n",
      "Comparing 29000\n",
      "Comparing 29500\n",
      "Comparing 30000\n",
      "Comparing 30500\n",
      "Comparing 31000\n",
      "Comparing 31500\n",
      "Comparing 32000\n",
      "Comparing 32500\n",
      "Comparing 33000\n",
      "Comparing 33500\n",
      "Comparing 34000\n",
      "Comparing 34500\n",
      "Comparing 35000\n",
      "Comparing 35500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-20e86125fdaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Comparing'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinHash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_perm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_littlesis_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\samar\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\datasketch\\minhash.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_perm, seed, hashfunc, hashobj, hashvalues, permutations)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;31m# that maps a 32-bit hash value to another 32-bit hash value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;31m# http://en.wikipedia.org/wiki/Universal_hashing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             self.permutations = np.array([(generator.randint(1, _mersenne_prime, dtype=np.uint64),\n\u001b[0m\u001b[0;32m     94\u001b[0m                                            generator.randint(0, _mersenne_prime, dtype=np.uint64))\n\u001b[0;32m     95\u001b[0m                                           for _ in range(num_perm)], dtype=np.uint64).T\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "matched_names = {}\n",
    "\n",
    "for name_id in clean_littlesis_names:  # for each name in littlesis data, find matches in the hash\n",
    "    count += 1\n",
    "    if count % 500 == 0:\n",
    "        print('Comparing', count)\n",
    "        \n",
    "    p = MinHash(num_perm = 128)\n",
    "    \n",
    "    name = clean_littlesis_names[name_id]\n",
    "    \n",
    "    for name_split in name.split():\n",
    "        p.update(name_split.encode('utf-8'))\n",
    "    \n",
    "    res = lsh.query(p)\n",
    "    \n",
    "    for bip_name in res:\n",
    "        same_person = compareNames(name, bip_name)\n",
    "        \n",
    "        if same_person:\n",
    "            matched_names[name_id] = clean_bipartite_names_reverse[bip_name]\n",
    "\n",
    "print(len(matched_names))\n",
    "\n",
    "# with open(os.getcwd() + '/Pickle/matched_names.pickle', 'wb') as pkl:\n",
    "#     pickle.dump(matched_names, pkl, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
