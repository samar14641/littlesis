{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from nameparser import HumanName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read bipartite graph and extract names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4054832"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipartite_all = nx.read_gpickle(os.getcwd() + '/Pickle/bipartite-all010420.pickle')\n",
    "bipartite_all.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes from *bipartite_all* have the following attributes:\n",
    "1. degree: int\n",
    "2. name: string\n",
    "3. is_trusted: bool\n",
    "4. is_employee: bool\n",
    "5. bipartite: int\n",
    "\n",
    "Access: bipartite_all.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'degree': 17,\n",
       " 'name': 'JOSEPH GALLO',\n",
       " 'is_trustee': True,\n",
       " 'is_employee': True,\n",
       " 'bipartite': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipartite_all.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'JOSEPH GALLO'),\n",
       " (1, 'EMMA DUNCH'),\n",
       " (2, 'DONALD BORROR'),\n",
       " (3, 'LINDA MOFFITT'),\n",
       " (4, \"M'LISS DORRANCE\")]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipartite_names = list(bipartite_all.nodes(data = 'name'))  # extract only names\n",
    "bipartite_names[: 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read LittleSis names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205548"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "littlesis_names = {}\n",
    "\n",
    "with open(os.getcwd() + '/Pickle/all_people.pickle', 'rb') as pkl:\n",
    "    littlesis_names = pickle.load(pkl)\n",
    "    \n",
    "len(littlesis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Roland A Hernandez'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "littlesis_names['1024']  # key is an ent ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNames(names_dict):\n",
    "    clean_names = {}\n",
    "    error_count, error_list = 0, []\n",
    "    \n",
    "    for name_id in names_dict:\n",
    "        try:\n",
    "            name = names_dict[name_id].strip().upper()\n",
    "\n",
    "            human_name = HumanName(name)\n",
    "\n",
    "            names_list = [human_name.first, human_name.middle, human_name.last]\n",
    "            names_list = [n for n in names_list if len(n) > 0]\n",
    "\n",
    "            if type(name_id) == str:  # convert string ent IDs to int\n",
    "                name_id = int(name_id)\n",
    "                \n",
    "            clean_names[name_id] = ' '.join(names_list)\n",
    "        \n",
    "        except:\n",
    "            error_count += 1\n",
    "            error_list.append(name_id)\n",
    "            \n",
    "#     print(error_count, error_list[: 3])\n",
    "    print(error_count)\n",
    "    return clean_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clean_littlesis_names = cleanNames(littlesis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROLAND A HERNANDEZ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_littlesis_names[1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "max_num = 1000  # max number of names from bipartite_all, change this value to change the size of the bipartite_all subset!\n",
    "# max_num = len(bipartite_names)  # select this to run for all names in bipartite-all\n",
    "bipartite_names_temp = {item[0]: item[1] for item in bipartite_names[: max_num] if item[1] is not None}  # create dict with ID:name for bipartite-all people if their name is not None, ID is the same as node ID in the nx object\n",
    "clean_bipartite_names = cleanNames(bipartite_names_temp)\n",
    "clean_bipartite_names_reverse = {clean_bipartite_names[i]: i for i in clean_bipartite_names}\n",
    "remaining_bipartite = len(clean_bipartite_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205548 740 740\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_littlesis_names), len(clean_bipartite_names), len(clean_bipartite_names_reverse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting 100 out of 740\n",
      "Inserting 200 out of 740\n",
      "Inserting 300 out of 740\n",
      "Inserting 400 out of 740\n",
      "Inserting 500 out of 740\n",
      "Inserting 600 out of 740\n",
      "Inserting 700 out of 740\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "lsh = MinHashLSH(threshold = 0.5, num_perm = 128)\n",
    "\n",
    "for name_id in clean_bipartite_names:\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print('Inserting', count, 'out of', remaining_bipartite)\n",
    "    \n",
    "    p = MinHash(num_perm = 128)\n",
    "    \n",
    "    name = clean_bipartite_names[name_id]\n",
    "    \n",
    "    for name_split in name.split():\n",
    "        p.update(name_split.encode('utf-8'))\n",
    "        \n",
    "    lsh.insert(name, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 740 names from bipartite-all\n"
     ]
    }
   ],
   "source": [
    "del clean_bipartite_names\n",
    "print('Inserted', count, 'names from bipartite-all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareNames(person1, person2):\n",
    "    same_person_flag = False\n",
    "    \n",
    "    if person1 != person2:\n",
    "        names1 = person1.split()\n",
    "        names2 = person2.split()\n",
    "        \n",
    "        common_names = list(set(names1) & set(names2))  # intersection\n",
    "        \n",
    "        # we need names of min len = 3 to be same\n",
    "        common_names_min_len = [n for n in common_names if len(n) >= 3]\n",
    "        \n",
    "        neither = None\n",
    "        \n",
    "        try:\n",
    "            if len(common_names_min_len) > 1:  # if at least 2 names satisfy min len condition\n",
    "                if len(common_names) == len(names1) or len(common_names) == len(names2):\n",
    "                    same_person_flag = True\n",
    "\n",
    "                elif (len(names1) - len(common_names) == 1 or len(names2) - len(common_names) == 1) and (len(names1) - len(common_names) <= 2 or len(names2) - len(common_names) <= 2) and names1[0] == names2[0] and names1[-1] == names2[-1]:\n",
    "                    neither = list(set(names1).union(set(names2)) - (set(names1) & set(names2)))  # neither i.e. complement = union - intersection\n",
    "\n",
    "                    if len(neither) == 2 and (len(neither[0]) == 1 or len(neither[1] == 1)) and neither[0][0] == neither[1][0]:\n",
    "                        same_person_flag = True\n",
    "\n",
    "        except:\n",
    "            print(neither, type(neither), person1)\n",
    "    \n",
    "    elif person1 == person2:\n",
    "        same_person_flag = True\n",
    "    \n",
    "    return same_person_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "matched_names = {}\n",
    "\n",
    "for name_id in clean_littlesis_names:  # for each name in littlesis data, find matches in the hash\n",
    "    count += 1\n",
    "    if count % 500 == 0:\n",
    "        print('Comparing', count)\n",
    "        \n",
    "    p = MinHash(num_perm = 128)\n",
    "    \n",
    "    name = clean_littlesis_names[name_id]\n",
    "    \n",
    "    for name_split in name.split():\n",
    "        p.update(name_split.encode('utf-8'))\n",
    "    \n",
    "    res = lsh.query(p)\n",
    "    \n",
    "    for bip_name in res:\n",
    "        same_person = compareNames(name, bip_name)\n",
    "        \n",
    "        if same_person:\n",
    "#             matched_names[name_id] = clean_bipartite_names_reverse[bip_name]\n",
    "            # handling multiple matches\n",
    "            if name_id in matched_names:\n",
    "                matched_names[name_id].append(clean_bipartite_names_reverse[bip_name])  # append if we've seen at least one match for name_id before\n",
    "            else:\n",
    "                matched_names[name_id] = [clean_bipartite_names_reverse[bip_name]]  # create a new list if it's the first match for name_id\n",
    "\n",
    "print(len(matched_names))\n",
    "\n",
    "with open(os.getcwd() + '/Pickle/matched_names.pickle', 'wb') as pkl:\n",
    "    pickle.dump(matched_names, pkl, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118659"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_names_res = {}\n",
    "\n",
    "with open(os.getcwd() + '/Pickle/matched_names.pickle', 'rb') as pkl:\n",
    "    matched_names_res = pickle.load(pkl)\n",
    "    \n",
    "len(matched_names_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[934460, 3191192]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_names_res[262150]  # key: LittleSis ent ID, value: list with node IDs from bipartite-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THOMAS ROONEY'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_littlesis_names[262150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'degree': 6,\n",
       " 'name': 'THOMAS J ROONEY',\n",
       " 'is_trustee': True,\n",
       " 'is_employee': True,\n",
       " 'bipartite': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipartite_all.nodes[934460]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
